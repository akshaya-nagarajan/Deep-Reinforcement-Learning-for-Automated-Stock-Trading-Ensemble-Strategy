{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "RL_Ensemble_3algos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLPb5OhMsIV1"
      },
      "source": [
        "# CMPE - 297 - SEC 47\n",
        "# Reinforcement Learning Project\n",
        "## Team Members - Akshaya Nagarajan, Pooja Patil\n",
        "## Automated Stock Trading using Deep Reinforcement Learning\n",
        "#### In this project we implement an Ensemble Strategy using Reinforcement Learning to trade stocks through Open AI Gym environment with an end goal of maximizing total returns.\n",
        "#### Reference Hongyang Yang, Xiao-Yang Liu, Shan Zhong, and Anwar Walid. 2020. Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy. In ICAIF ’20: ACM International Conference on AI in Finance, Oct. 15–16, 2020, Manhattan, NY. ACM, New York, NY, USA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5wqqrUEsYe1"
      },
      "source": [
        "## Mount GDrive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgg2OMQnT8Fp",
        "outputId": "6e9ed73c-905e-42fd-d2e9-5a81685f8121"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/RLProject\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqSxZq5Usbpw"
      },
      "source": [
        "## Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5Dxloq409GQ",
        "outputId": "e5ad32c8-2c3f-4171-84c8-17d1c46fc874"
      },
      "source": [
        "!pip install stockstats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stockstats in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from stockstats) (1.1.4)\n",
            "Requirement already satisfied: int-date>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from stockstats) (0.1.8)\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from stockstats) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.1->stockstats) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18.1->stockstats) (2.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from int-date>=0.1.7->stockstats) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhihNn731FAi",
        "outputId": "b5cc5bfb-2d2d-41c8-a137-efd886f9a689"
      },
      "source": [
        "!pip install yfinance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.6/dist-packages (0.1.55)\n",
            "Requirement already satisfied: lxml>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from yfinance) (4.6.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.6/dist-packages (from yfinance) (1.1.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.6/dist-packages (from yfinance) (2.23.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from yfinance) (0.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24->yfinance) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.20->yfinance) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24->yfinance) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWh9GwHQ1JCu",
        "outputId": "9cc0791f-44a2-4e41-9d4b-75e42166308d"
      },
      "source": [
        "!pip install git+https://github.com/quantopian/pyfolio"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/quantopian/pyfolio\n",
            "  Cloning https://github.com/quantopian/pyfolio to /tmp/pip-req-build-okqbcdma\n",
            "  Running command git clone -q https://github.com/quantopian/pyfolio /tmp/pip-req-build-okqbcdma\n",
            "Requirement already satisfied (use --upgrade to upgrade): pyfolio==0.9.2+75.g4b901f6 from git+https://github.com/quantopian/pyfolio in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (5.5.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (1.1.4)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (0.22.2.post1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (0.11.0)\n",
            "Requirement already satisfied: empyrical>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from pyfolio==0.9.2+75.g4b901f6) (0.5.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (2.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (4.3.3)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (50.3.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->pyfolio==0.9.2+75.g4b901f6) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->pyfolio==0.9.2+75.g4b901f6) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->pyfolio==0.9.2+75.g4b901f6) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.0->pyfolio==0.9.2+75.g4b901f6) (2.8.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16.1->pyfolio==0.9.2+75.g4b901f6) (0.17.0)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.6/dist-packages (from empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (0.9.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.6.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=3.2.3->pyfolio==0.9.2+75.g4b901f6) (0.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (4.6.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio==0.9.2+75.g4b901f6) (2.10)\n",
            "Building wheels for collected packages: pyfolio\n",
            "  Building wheel for pyfolio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfolio: filename=pyfolio-0.9.2+75.g4b901f6-cp36-none-any.whl size=75764 sha256=f521f9078e680eec309539b9a1c9115513e17841e51ffb9dc07412ee63d5934b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4qcgb2x9/wheels/62/7d/a7/3e462442ba7d63c35414176627c886340521dc3dbc0893ce9f\n",
            "Successfully built pyfolio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-9VeWyb1aYj",
        "outputId": "d9ef66df-bcb4-4102-9303-2553403dff97"
      },
      "source": [
        "!pip install gym    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g32_IC-N1kBl",
        "outputId": "4a65a77f-67b5-4c86-8d55-8ab729ea41a9"
      },
      "source": [
        "!pip install stable-baselines[mpi]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stable-baselines[mpi] in /usr/local/lib/python3.6/dist-packages (2.10.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (3.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.1.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (4.1.2.30)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (1.3.0)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]) (3.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]) (2018.9)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->stable-baselines[mpi]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTRGB1OX1mrS",
        "outputId": "7be0e791-93b3-4280-ab7d-1b708af658a8"
      },
      "source": [
        "!pip install tensorflow==1.15.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.15.4 in /usr/local/lib/python3.6/dist-packages (1.15.4)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.10.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.18.5)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.35.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.12.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (1.33.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.4) (0.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (50.3.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.4) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.4) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CstwA99sfoQ"
      },
      "source": [
        "## Import required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WpB6I-vcuHx",
        "outputId": "b8714557-5786-4b49-c202-9c65e59724d4"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from gym.utils import seeding\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from stockstats import StockDataFrame as Sdf\n",
        "import yfinance as yf\n",
        "import pyfolio\n",
        "\n",
        "from stable_baselines import PPO2, DDPG, A2C, ACKTR, TD3, TRPO\n",
        "from stable_baselines import DDPG\n",
        "from stable_baselines import A2C\n",
        "from stable_baselines import SAC\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.common.policies import MlpPolicy\n",
        "from stable_baselines.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3v2pX2gskQL"
      },
      "source": [
        "## Data Files Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi3yJ3AWT9d-"
      },
      "source": [
        "TRAINING_DATA_FILE = root_dir + '/data/dow_30_2009_2020.csv'\n",
        "TURBULENCE_DATA = root_dir + '/data/dow30_turbulence_index.csv'\n",
        "TRAINED_MODEL_DIR = root_dir + '/trained_models'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IspIiTKesnM2"
      },
      "source": [
        "## List of Stocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv7nqVMWwgB7"
      },
      "source": [
        "dow_30_ticker = ['AAPL','MSFT','JPM','V','RTX','PG','GS','NKE','DIS','AXP',\n",
        "                  'HD','INTC','WMT','IBM','MRK','UNH','KO','CAT','TRV','JNJ',\n",
        "                  'CVX','MCD','VZ','CSCO','XOM','BA','MMM','PFE','WBA','DD']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG34UqUbsq8F"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVKwXq0hwjW5"
      },
      "source": [
        "def load_dataset_dow30() -> pd.DataFrame:\n",
        "  dow_30 = pd.DataFrame()\n",
        "  for tic in dow_30_ticker:\n",
        "      data_df = yf.download(tic, start=\"2009-01-01\", end=\"2020-10-23\")\n",
        "      data_df['tic'] = tic\n",
        "      dow_30=dow_30.append(data_df)\n",
        "\n",
        "  dow_30=dow_30.reset_index()\n",
        "  print(dow_30.columns)\n",
        "  dow_30.rename(columns={\"Date\": \"datadate\", \"Open\": \"open\", \"High\": \"high\", \"Low\": \"low\", \"Close\": \"close\", \"Adj Close\": \"adjcp\", \"Volume\": \"volume\", \"tic\": \"tic\"}, inplace = True)\n",
        "  print(dow_30.columns)\n",
        "  #dow_30.columns = ['datadate','prcod','prchd','prcld','prccd','adjcp','cshtrd','tic']\n",
        "  #['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'tic']\n",
        "  return dow_30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnNuvQnKclFN"
      },
      "source": [
        "def load_dataset(file_name: str) -> pd.DataFrame:\n",
        "    _data = pd.read_csv(file_name)\n",
        "    return _data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5wyV4iTstzo"
      },
      "source": [
        "## Data split based on dates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnCFrv2krVM2"
      },
      "source": [
        "def data_split(df,start,end):\n",
        "    data = df[(df.datadate >= start) & (df.datadate < end)]\n",
        "    data=data.sort_values(['datadate','tic'],ignore_index=True)\n",
        "    data.index = data.datadate.factorize()[0]\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR05OZdgs0kK"
      },
      "source": [
        "## Calculate values - datadate, tic, adjcp, open, high, low, volume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahzIq5CgrZsR"
      },
      "source": [
        "def calcualte_price(df):\n",
        "    data = df.copy()\n",
        "    data = data[['datadate', 'tic', 'prccd', 'ajexdi', 'prcod', 'prchd', 'prcld', 'cshtrd']]\n",
        "    data['ajexdi'] = data['ajexdi'].apply(lambda x: 1 if x == 0 else x)\n",
        "    data['adjcp'] = data['prccd'] / data['ajexdi']\n",
        "    data['open'] = data['prcod'] / data['ajexdi']\n",
        "    data['high'] = data['prchd'] / data['ajexdi']\n",
        "    data['low'] = data['prcld'] / data['ajexdi']\n",
        "    data['volume'] = data['cshtrd']\n",
        "    data = data[['datadate', 'tic', 'adjcp', 'open', 'high', 'low', 'volume']]\n",
        "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si7wRzrF9E3W"
      },
      "source": [
        "def calcualte_price_dow30(df):\n",
        "    data = df.copy()\n",
        "    data = data[['datadate', 'tic', 'adjcp', 'open', 'high', 'low', 'volume']]\n",
        "    data = data.sort_values(['tic', 'datadate'], ignore_index=True)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8uVD9HLtBE1"
      },
      "source": [
        "## Including Technical indicators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuWH-eMXrhFS"
      },
      "source": [
        "def add_technical_indicator(df):\n",
        "    stock = Sdf.retype(df.copy())\n",
        "    stock['close'] = stock['adjcp']\n",
        "    unique_ticker = stock.tic.unique()\n",
        "    macd = pd.DataFrame()\n",
        "    rsi = pd.DataFrame()\n",
        "    cci = pd.DataFrame()\n",
        "    dx = pd.DataFrame()\n",
        "\n",
        "    for i in range(len(unique_ticker)):\n",
        "        ## macd\n",
        "        temp_macd = stock[stock.tic == unique_ticker[i]]['macd']\n",
        "        temp_macd = pd.DataFrame(temp_macd)\n",
        "        macd = macd.append(temp_macd, ignore_index=True)\n",
        "        ## rsi\n",
        "        temp_rsi = stock[stock.tic == unique_ticker[i]]['rsi_30']\n",
        "        temp_rsi = pd.DataFrame(temp_rsi)\n",
        "        rsi = rsi.append(temp_rsi, ignore_index=True)\n",
        "        ## cci\n",
        "        temp_cci = stock[stock.tic == unique_ticker[i]]['cci_30']\n",
        "        temp_cci = pd.DataFrame(temp_cci)\n",
        "        cci = cci.append(temp_cci, ignore_index=True)\n",
        "        ## adx\n",
        "        temp_dx = stock[stock.tic == unique_ticker[i]]['dx_30']\n",
        "        temp_dx = pd.DataFrame(temp_dx)\n",
        "        dx = dx.append(temp_dx, ignore_index=True)\n",
        "\n",
        "\n",
        "    df['macd'] = macd\n",
        "    df['rsi'] = rsi\n",
        "    df['cci'] = cci\n",
        "    df['adx'] = dx\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUBI9tNutFoh"
      },
      "source": [
        "## Preprocess data \n",
        "### Removing null values, filling values that are empty in technical indicators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkZJl0QdrqMc"
      },
      "source": [
        "def preprocess_data():\n",
        "    df = load_dataset(TRAINING_DATA_FILE)\n",
        "    #df = load_dataset_dow30()\n",
        "\n",
        "    print(df.head(2))\n",
        "\n",
        "    # Null checks\n",
        "    print(df.isnull().values.any())\n",
        "    df = df.dropna()\n",
        "    print(df.tic.value_counts())\n",
        "    print(df.isnull().values.any())\n",
        "    \n",
        "    # get data after 2009\n",
        "    df = df[df.datadate>=20090000]\n",
        "    #df = df[df.datadate>=\"2009-01-01\"]\n",
        "    \n",
        "    # calcualte adjusted price\n",
        "    df_preprocess = calcualte_price(df)\n",
        "    #df_preprocess = calcualte_price_dow30(df)\n",
        "    # add technical indicators using stockstats\n",
        "\n",
        "    df_final=add_technical_indicator(df_preprocess)\n",
        "    print(df_final.isnull().values.any())\n",
        "    print(df_final.isna().any())\n",
        "    print(df_final.tail(3))\n",
        "    \n",
        "    # fill the missing values at the beginning\n",
        "    df_final.fillna(method='bfill',inplace=True)\n",
        "    print(df_final.tail(3))\n",
        "    \n",
        "    return df_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUQ6atyxtVzl"
      },
      "source": [
        "## Calculation for adding turbulence index to data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pv5cpVsrvY0"
      },
      "source": [
        "def add_turbulence(df):\n",
        "    turbulence_index = calcualte_turbulence(df)\n",
        "    df = df.merge(turbulence_index, on='datadate')\n",
        "    df = df.sort_values(['datadate','tic']).reset_index(drop=True)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irqZB1Kjry_5"
      },
      "source": [
        "def calcualte_turbulence(df):\n",
        "    df_price_pivot=df.pivot(index='datadate', columns='tic', values='adjcp')\n",
        "    unique_date = df.datadate.unique()\n",
        "    # start after a year\n",
        "    start = 252\n",
        "    turbulence_index = [0]*start\n",
        "    #turbulence_index = [0]\n",
        "    count=0\n",
        "    for i in range(start,len(unique_date)):\n",
        "        current_price = df_price_pivot[df_price_pivot.index == unique_date[i]]\n",
        "        hist_price = df_price_pivot[[n in unique_date[0:i] for n in df_price_pivot.index ]]\n",
        "        cov_temp = hist_price.cov()\n",
        "        current_temp=(current_price - np.mean(hist_price,axis=0))\n",
        "        temp = current_temp.values.dot(np.linalg.inv(cov_temp)).dot(current_temp.values.T)\n",
        "        if temp>0:\n",
        "            count+=1\n",
        "            if count>2:\n",
        "                turbulence_temp = temp[0][0]\n",
        "            else:\n",
        "                #avoid large outlier because of the calculation just begins\n",
        "                turbulence_temp=0\n",
        "        else:\n",
        "            turbulence_temp=0\n",
        "        turbulence_index.append(turbulence_temp)\n",
        "    \n",
        "    \n",
        "    turbulence_index = pd.DataFrame({'datadate':df_price_pivot.index, 'turbulence':turbulence_index})\n",
        "    return turbulence_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGy-7TGTirWI"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU76Vc5Gdp_X"
      },
      "source": [
        "# 100 shares per trade\n",
        "MAX_NORMALIZE = 100\n",
        "# initial amount of money we have in account\n",
        "INITIAL_ACC_BAL=1000000\n",
        "# total number of stocks in  portfolio\n",
        "TOTAL_STOCKS = 30\n",
        "# transaction fee: 1/1000 reasonable percentage\n",
        "TRANSACTION_FEE_PERCENT = 0.001\n",
        "REWARD_SCALING = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxHWCaoZuNGW"
      },
      "source": [
        "## Gym Environments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo7DwGmleT-p"
      },
      "source": [
        "class StockTrainEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df,day = 0):\n",
        "        #money = 10 , scope = 1\n",
        "        self.day = day\n",
        "        self.df = df\n",
        "\n",
        "        # action_space \n",
        "        self.action_space = spaces.Box(low = -1, high = 1,shape = (TOTAL_STOCKS,)) \n",
        "        \n",
        "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
        "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
        "        \n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.terminal = False             \n",
        "        \n",
        "        # initalize state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist() + \\\n",
        "                      self.data.cci.values.tolist() + \\\n",
        "                      self.data.adx.values.tolist()\n",
        "        # initialize reward\n",
        "        self.reward = 0\n",
        "        self.cost = 0\n",
        "        # memorize all the total balance change\n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.rewards_memory = []\n",
        "        self.trades = 0\n",
        "        #self.reset()\n",
        "        self._seed()\n",
        "\n",
        "\n",
        "    def _sell_stock(self, index, action):\n",
        "        # perform sell action based on the sign of the action\n",
        "        if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "            #update balance\n",
        "            self.state[0] += \\\n",
        "            self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "             (1- TRANSACTION_FEE_PERCENT)\n",
        "\n",
        "            self.state[index+TOTAL_STOCKS+1] -= min(abs(action), self.state[index+TOTAL_STOCKS+1])\n",
        "            self.cost +=self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "             TRANSACTION_FEE_PERCENT\n",
        "            self.trades+=1\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    \n",
        "    def _buy_stock(self, index, action):\n",
        "        # perform buy action based on the sign of the action\n",
        "        available_amount = self.state[0] // self.state[index+1]\n",
        "        # print('available_amount:{}'.format(available_amount))\n",
        "\n",
        "        #update balance\n",
        "        self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
        "                          (1+ TRANSACTION_FEE_PERCENT)\n",
        "\n",
        "        self.state[index+TOTAL_STOCKS+1] += min(available_amount, action)\n",
        "\n",
        "        self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
        "                          TRANSACTION_FEE_PERCENT\n",
        "        self.trades+=1\n",
        "        \n",
        "    def step(self, actions):\n",
        "        # print(self.day)\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "        # print(actions)\n",
        "\n",
        "        if self.terminal:\n",
        "            plt.plot(self.asset_memory,'r')\n",
        "            plt.savefig('account_value_train.png')\n",
        "            plt.close()\n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            \n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            df_total_value = pd.DataFrame(self.asset_memory)\n",
        "            df_total_value.to_csv('account_value_train.csv')\n",
        "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):61]))- INITIAL_ACCOUNT_BALANCE ))\n",
        "            #print(\"total_cost: \", self.cost)\n",
        "            #print(\"total_trades: \", self.trades)\n",
        "            df_total_value.columns = ['account_value']\n",
        "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
        "            sharpe = (252**0.5)*df_total_value['daily_return'].mean()/ \\\n",
        "                  df_total_value['daily_return'].std()\n",
        "            #print(\"Sharpe: \",sharpe)\n",
        "            #print(\"=================================\")\n",
        "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
        "            #df_rewards.to_csv('account_rewards_train.csv')\n",
        "            \n",
        "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
        "            #with open('obs.pkl', 'wb') as f:  \n",
        "            #    pickle.dump(self.state, f)\n",
        "            \n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            # print(np.array(self.state[1:29]))\n",
        "\n",
        "            actions = actions * MAX_NORMALIZE\n",
        "            #actions = (actions.astype(int))\n",
        "            \n",
        "            begin_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
        "            \n",
        "            argsort_actions = np.argsort(actions)\n",
        "            \n",
        "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
        "\n",
        "            for index in sell_index:\n",
        "                # print('take sell action'.format(actions[index]))\n",
        "                self._sell_stock(index, actions[index])\n",
        "\n",
        "            for index in buy_index:\n",
        "                # print('take buy action: {}'.format(actions[index]))\n",
        "                self._buy_stock(index, actions[index])\n",
        "\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]         \n",
        "            #load next state\n",
        "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
        "            self.state =  [self.state[0]] + \\\n",
        "                    self.data.adjcp.values.tolist() + \\\n",
        "                    list(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]) + \\\n",
        "                    self.data.macd.values.tolist() + \\\n",
        "                    self.data.rsi.values.tolist() + \\\n",
        "                    self.data.cci.values.tolist() + \\\n",
        "                    self.data.adx.values.tolist()\n",
        "            \n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            self.asset_memory.append(end_total_asset)\n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            \n",
        "            self.reward = end_total_asset - begin_total_asset            \n",
        "            # print(\"step_reward:{}\".format(self.reward))\n",
        "            self.rewards_memory.append(self.reward)\n",
        "            \n",
        "            self.reward = self.reward*REWARD_SCALING\n",
        "\n",
        "\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        self.terminal = False \n",
        "        self.rewards_memory = []\n",
        "        #initiate state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist() + \\\n",
        "                      self.data.cci.values.tolist() + \\\n",
        "                      self.data.adx.values.tolist() \n",
        "        # iteration += 1 \n",
        "        return self.state\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        return self.state\n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Whz5WMXbeWED"
      },
      "source": [
        "class StockValidationEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df, day = 0, turbulence_threshold=140, iteration=''):\n",
        "        \n",
        "        #money = 10 , scope = 1\n",
        "        self.day = day\n",
        "        self.df = df\n",
        "        \n",
        "        # action_space normalization and shape is TOTAL_STOCKS\n",
        "        self.action_space = spaces.Box(low = -1, high = 1,shape = (TOTAL_STOCKS,)) \n",
        "        \n",
        "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
        "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.terminal = False     \n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        \n",
        "        # initalize state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist() + \\\n",
        "                      self.data.cci.values.tolist() + \\\n",
        "                      self.data.adx.values.tolist()\n",
        "        # initialize reward\n",
        "        self.reward = 0\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        # memorize all the total balance change\n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.rewards_memory = []\n",
        "        #self.reset()\n",
        "        self._seed()\n",
        "        \n",
        "        self.iteration=iteration\n",
        "\n",
        "\n",
        "    def _sell_stock(self, index, action):\n",
        "        # perform sell action based on the sign of the action\n",
        "        if self.turbulence<self.turbulence_threshold:\n",
        "            if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "                #update balance\n",
        "                self.state[0] += \\\n",
        "                self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "                 (1- TRANSACTION_FEE_PERCENT)\n",
        "                \n",
        "                self.state[index+TOTAL_STOCKS+1] -= min(abs(action), self.state[index+TOTAL_STOCKS+1])\n",
        "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "                 TRANSACTION_FEE_PERCENT\n",
        "                self.trades+=1\n",
        "            else:\n",
        "                pass\n",
        "        else:\n",
        "            # if turbulence goes over threshold, just clear out all positions \n",
        "            if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "                #update balance\n",
        "                self.state[0] += self.state[index+1]*self.state[index+TOTAL_STOCKS+1]* \\\n",
        "                              (1- TRANSACTION_FEE_PERCENT)\n",
        "                self.state[index+TOTAL_STOCKS+1] =0\n",
        "                self.cost += self.state[index+1]*self.state[index+TOTAL_STOCKS+1]* \\\n",
        "                              TRANSACTION_FEE_PERCENT\n",
        "                self.trades+=1\n",
        "            else:\n",
        "                pass\n",
        "    \n",
        "    def _buy_stock(self, index, action):\n",
        "        # perform buy action based on the sign of the action\n",
        "        if self.turbulence< self.turbulence_threshold:\n",
        "            available_amount = self.state[0] // self.state[index+1]\n",
        "            # print('available_amount:{}'.format(available_amount))\n",
        "            \n",
        "            #update balance\n",
        "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
        "                              (1+ TRANSACTION_FEE_PERCENT)\n",
        "\n",
        "            self.state[index+TOTAL_STOCKS+1] += min(available_amount, action)\n",
        "            \n",
        "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
        "                              TRANSACTION_FEE_PERCENT\n",
        "            self.trades+=1\n",
        "        else:\n",
        "            # if turbulence goes over threshold, just stop buying\n",
        "            pass\n",
        "        \n",
        "    def step(self, actions):\n",
        "        # print(self.day)\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "        # print(actions)\n",
        "\n",
        "        if self.terminal:\n",
        "            plt.plot(self.asset_memory,'r')\n",
        "            plt.savefig('account_value_validation_{}.png'.format(self.iteration))\n",
        "            plt.close()\n",
        "            df_total_value = pd.DataFrame(self.asset_memory)\n",
        "            df_total_value.to_csv('account_value_validation_{}.csv'.format(self.iteration))\n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            #print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
        "\n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            #print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):61]))- self.asset_memory[0] ))\n",
        "            #print(\"total_cost: \", self.cost)\n",
        "            #print(\"total trades: \", self.trades)\n",
        "\n",
        "            df_total_value.columns = ['account_value']\n",
        "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
        "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
        "                  df_total_value['daily_return'].std()\n",
        "            #print(\"Sharpe: \",sharpe)\n",
        "            \n",
        "            #df_rewards = pd.DataFrame(self.rewards_memory)\n",
        "            #df_rewards.to_csv('account_rewards_trade_{}.csv'.format(self.iteration))\n",
        "            \n",
        "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
        "            #with open('obs.pkl', 'wb') as f:  \n",
        "            #    pickle.dump(self.state, f)\n",
        "            \n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            # print(np.array(self.state[1:29]))\n",
        "\n",
        "            actions = actions * MAX_NORMALIZE\n",
        "            #actions = (actions.astype(int))\n",
        "            if self.turbulence>=self.turbulence_threshold:\n",
        "                actions=np.array([-MAX_NORMALIZE]*TOTAL_STOCKS)\n",
        "            begin_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
        "            \n",
        "            argsort_actions = np.argsort(actions)\n",
        "            \n",
        "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
        "\n",
        "            for index in sell_index:\n",
        "                # print('take sell action'.format(actions[index]))\n",
        "                self._sell_stock(index, actions[index])\n",
        "\n",
        "            for index in buy_index:\n",
        "                # print('take buy action: {}'.format(actions[index]))\n",
        "                self._buy_stock(index, actions[index])\n",
        "\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]         \n",
        "            self.turbulence = self.data['turbulence'].values[0]\n",
        "            #print(self.turbulence)\n",
        "            #load next state\n",
        "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
        "            self.state =  [self.state[0]] + \\\n",
        "                    self.data.adjcp.values.tolist() + \\\n",
        "                    list(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]) + \\\n",
        "                    self.data.macd.values.tolist() + \\\n",
        "                    self.data.rsi.values.tolist() + \\\n",
        "                    self.data.cci.values.tolist() + \\\n",
        "                    self.data.adx.values.tolist()\n",
        "            \n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            self.asset_memory.append(end_total_asset)\n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            \n",
        "            self.reward = end_total_asset - begin_total_asset            \n",
        "            # print(\"step_reward:{}\".format(self.reward))\n",
        "            self.rewards_memory.append(self.reward)\n",
        "            \n",
        "            self.reward = self.reward*REWARD_SCALING\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):  \n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.day = 0\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        self.terminal = False \n",
        "        #self.iteration=self.iteration\n",
        "        self.rewards_memory = []\n",
        "        #initiate state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist()  + \\\n",
        "                      self.data.cci.values.tolist()  + \\\n",
        "                      self.data.adx.values.tolist() \n",
        "            \n",
        "        return self.state\n",
        "    \n",
        "    def render(self, mode='human',close=False):\n",
        "        return self.state\n",
        "    \n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c3qjJUVeYo6"
      },
      "source": [
        "class StockTradeEnv(gym.Env):\n",
        "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df,day = 0,turbulence_threshold=140\n",
        "                 ,initial=True, previous_state=[], model_name='', iteration=''):\n",
        "        #money = 10 , scope = 1\n",
        "        self.day = day\n",
        "        self.df = df\n",
        "        self.initial = initial\n",
        "        self.previous_state = previous_state\n",
        "        \n",
        "        # action_space normalization and shape is TOTAL_STOCKS\n",
        "        self.action_space = spaces.Box(low = -1, high = 1,shape = (TOTAL_STOCKS,)) \n",
        "        \n",
        "        # Shape = 181: [Current Balance]+[prices 1-30]+[owned shares 1-30] \n",
        "        # +[macd 1-30]+ [rsi 1-30] + [cci 1-30] + [adx 1-30]\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape = (181,))\n",
        "        \n",
        "        # load data from a pandas dataframe\n",
        "        self.data = self.df.loc[self.day,:]\n",
        "        self.terminal = False     \n",
        "        self.turbulence_threshold = turbulence_threshold\n",
        "        \n",
        "        # initalize state\n",
        "        self.state = [INITIAL_ACC_BAL] + \\\n",
        "                      self.data.adjcp.values.tolist() + \\\n",
        "                      [0]*TOTAL_STOCKS + \\\n",
        "                      self.data.macd.values.tolist() + \\\n",
        "                      self.data.rsi.values.tolist() + \\\n",
        "                      self.data.cci.values.tolist() + \\\n",
        "                      self.data.adx.values.tolist()\n",
        "        # initialize reward\n",
        "        self.reward = 0\n",
        "        self.turbulence = 0\n",
        "        self.cost = 0\n",
        "        self.trades = 0\n",
        "        # memorize all the total balance change\n",
        "        self.asset_memory = [INITIAL_ACC_BAL]\n",
        "        self.rewards_memory = []\n",
        "        #self.reset()\n",
        "        self._seed()\n",
        "        self.model_name=model_name        \n",
        "        self.iteration=iteration\n",
        "\n",
        "\n",
        "    def _sell_stock(self, index, action):\n",
        "        # perform sell action based on the sign of the action\n",
        "        if self.turbulence<self.turbulence_threshold:\n",
        "            if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "                #update balance\n",
        "                self.state[0] += \\\n",
        "                self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "                 (1- TRANSACTION_FEE_PERCENT)\n",
        "                \n",
        "                self.state[index+TOTAL_STOCKS+1] -= min(abs(action), self.state[index+TOTAL_STOCKS+1])\n",
        "                self.cost +=self.state[index+1]*min(abs(action),self.state[index+TOTAL_STOCKS+1]) * \\\n",
        "                 TRANSACTION_FEE_PERCENT\n",
        "                self.trades+=1\n",
        "            else:\n",
        "                pass\n",
        "        else:\n",
        "            # if turbulence goes over threshold, just clear out all positions \n",
        "            if self.state[index+TOTAL_STOCKS+1] > 0:\n",
        "                #update balance\n",
        "                self.state[0] += self.state[index+1]*self.state[index+TOTAL_STOCKS+1]* \\\n",
        "                              (1- TRANSACTION_FEE_PERCENT)\n",
        "                self.state[index+TOTAL_STOCKS+1] =0\n",
        "                self.cost += self.state[index+1]*self.state[index+TOTAL_STOCKS+1]* \\\n",
        "                              TRANSACTION_FEE_PERCENT\n",
        "                self.trades+=1\n",
        "            else:\n",
        "                pass\n",
        "    \n",
        "    def _buy_stock(self, index, action):\n",
        "        # perform buy action based on the sign of the action\n",
        "        if self.turbulence< self.turbulence_threshold:\n",
        "            available_amount = self.state[0] // self.state[index+1]\n",
        "            # print('available_amount:{}'.format(available_amount))\n",
        "            \n",
        "            #update balance\n",
        "            self.state[0] -= self.state[index+1]*min(available_amount, action)* \\\n",
        "                              (1+ TRANSACTION_FEE_PERCENT)\n",
        "\n",
        "            self.state[index+TOTAL_STOCKS+1] += min(available_amount, action)\n",
        "            \n",
        "            self.cost+=self.state[index+1]*min(available_amount, action)* \\\n",
        "                              TRANSACTION_FEE_PERCENT\n",
        "            self.trades+=1\n",
        "        else:\n",
        "            # if turbulence goes over threshold, just stop buying\n",
        "            pass\n",
        "        \n",
        "    def step(self, actions):\n",
        "        # print(self.day)\n",
        "        self.terminal = self.day >= len(self.df.index.unique())-1\n",
        "        # print(actions)\n",
        "\n",
        "        if self.terminal:\n",
        "            plt.plot(self.asset_memory,'r')\n",
        "            plt.savefig('account_value_trade_{}_{}.png'.format(self.model_name, self.iteration))\n",
        "            plt.close()\n",
        "            df_total_value = pd.DataFrame(self.asset_memory)\n",
        "            df_total_value.to_csv('account_value_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            print(\"previous_total_asset:{}\".format(self.asset_memory[0]))           \n",
        "\n",
        "            print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            print(\"total_reward:{}\".format(self.state[0]+sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))- self.asset_memory[0] ))\n",
        "            print(\"total_cost: \", self.cost)\n",
        "            print(\"total trades: \", self.trades)\n",
        "\n",
        "            df_total_value.columns = ['account_value']\n",
        "            df_total_value['daily_return']=df_total_value.pct_change(1)\n",
        "            sharpe = (4**0.5)*df_total_value['daily_return'].mean()/ \\\n",
        "                  df_total_value['daily_return'].std()\n",
        "            print(\"Sharpe: \",sharpe)\n",
        "            \n",
        "            df_rewards = pd.DataFrame(self.rewards_memory)\n",
        "            df_rewards.to_csv('account_rewards_trade_{}_{}.csv'.format(self.model_name, self.iteration))\n",
        "            \n",
        "            # print('total asset: {}'.format(self.state[0]+ sum(np.array(self.state[1:29])*np.array(self.state[29:]))))\n",
        "            #with open('obs.pkl', 'wb') as f:  \n",
        "            #    pickle.dump(self.state, f)\n",
        "            \n",
        "            return self.state, self.reward, self.terminal,{}\n",
        "\n",
        "        else:\n",
        "            # print(np.array(self.state[1:29]))\n",
        "\n",
        "            actions = actions * MAX_NORMALIZE\n",
        "            #actions = (actions.astype(int))\n",
        "            if self.turbulence>=self.turbulence_threshold:\n",
        "                actions=np.array([-MAX_NORMALIZE]*TOTAL_STOCKS)\n",
        "                \n",
        "            begin_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            #print(\"begin_total_asset:{}\".format(begin_total_asset))\n",
        "            \n",
        "            argsort_actions = np.argsort(actions)\n",
        "            \n",
        "            sell_index = argsort_actions[:np.where(actions < 0)[0].shape[0]]\n",
        "            buy_index = argsort_actions[::-1][:np.where(actions > 0)[0].shape[0]]\n",
        "\n",
        "            for index in sell_index:\n",
        "                # print('take sell action'.format(actions[index]))\n",
        "                self._sell_stock(index, actions[index])\n",
        "\n",
        "            for index in buy_index:\n",
        "                # print('take buy action: {}'.format(actions[index]))\n",
        "                self._buy_stock(index, actions[index])\n",
        "\n",
        "            self.day += 1\n",
        "            self.data = self.df.loc[self.day,:]         \n",
        "            self.turbulence = self.data['turbulence'].values[0]\n",
        "            #print(self.turbulence)\n",
        "            #load next state\n",
        "            # print(\"stock_shares:{}\".format(self.state[29:]))\n",
        "            self.state =  [self.state[0]] + \\\n",
        "                    self.data.adjcp.values.tolist() + \\\n",
        "                    list(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]) + \\\n",
        "                    self.data.macd.values.tolist() + \\\n",
        "                    self.data.rsi.values.tolist() + \\\n",
        "                    self.data.cci.values.tolist() + \\\n",
        "                    self.data.adx.values.tolist()\n",
        "            \n",
        "            end_total_asset = self.state[0]+ \\\n",
        "            sum(np.array(self.state[1:(TOTAL_STOCKS+1)])*np.array(self.state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            self.asset_memory.append(end_total_asset)\n",
        "            #print(\"end_total_asset:{}\".format(end_total_asset))\n",
        "            \n",
        "            self.reward = end_total_asset - begin_total_asset            \n",
        "            # print(\"step_reward:{}\".format(self.reward))\n",
        "            self.rewards_memory.append(self.reward)\n",
        "            \n",
        "            self.reward = self.reward*REWARD_SCALING\n",
        "\n",
        "\n",
        "        return self.state, self.reward, self.terminal, {}\n",
        "\n",
        "    def reset(self):  \n",
        "        if self.initial:\n",
        "            self.asset_memory = [INITIAL_ACC_BAL]\n",
        "            self.day = 0\n",
        "            self.data = self.df.loc[self.day,:]\n",
        "            self.turbulence = 0\n",
        "            self.cost = 0\n",
        "            self.trades = 0\n",
        "            self.terminal = False \n",
        "            #self.iteration=self.iteration\n",
        "            self.rewards_memory = []\n",
        "            #initiate state\n",
        "            self.state = [INITIAL_ACC_BAL] + \\\n",
        "                          self.data.adjcp.values.tolist() + \\\n",
        "                          [0]*TOTAL_STOCKS + \\\n",
        "                          self.data.macd.values.tolist() + \\\n",
        "                          self.data.rsi.values.tolist()  + \\\n",
        "                          self.data.cci.values.tolist()  + \\\n",
        "                          self.data.adx.values.tolist() \n",
        "        else:\n",
        "            previous_total_asset = self.previous_state[0]+ \\\n",
        "            sum(np.array(self.previous_state[1:(TOTAL_STOCKS+1)])*np.array(self.previous_state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]))\n",
        "            self.asset_memory = [previous_total_asset]\n",
        "            #self.asset_memory = [self.previous_state[0]]\n",
        "            self.day = 0\n",
        "            self.data = self.df.loc[self.day,:]\n",
        "            self.turbulence = 0\n",
        "            self.cost = 0\n",
        "            self.trades = 0\n",
        "            self.terminal = False \n",
        "            #self.iteration=iteration\n",
        "            self.rewards_memory = []\n",
        "            #initiate state\n",
        "            #self.previous_state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]\n",
        "            #[0]*TOTAL_STOCKS + \\\n",
        "\n",
        "            self.state = [ self.previous_state[0]] + \\\n",
        "                          self.data.adjcp.values.tolist() + \\\n",
        "                          self.previous_state[(TOTAL_STOCKS+1):(TOTAL_STOCKS*2+1)]+ \\\n",
        "                          self.data.macd.values.tolist() + \\\n",
        "                          self.data.rsi.values.tolist()  + \\\n",
        "                          self.data.cci.values.tolist()  + \\\n",
        "                          self.data.adx.values.tolist() \n",
        "            \n",
        "        return self.state\n",
        "    \n",
        "    def render(self, mode='human',close=False):\n",
        "        return self.state\n",
        "    \n",
        "\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_8q_FafuXCW"
      },
      "source": [
        "## Models used in  Ensemble Strategy - A2C, DDPG, PPO\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UInF7UItea9p"
      },
      "source": [
        " def train_A2C(env_train, model_name, timesteps=50000):\n",
        "    start = time.time()\n",
        "    model = A2C('MlpPolicy', env_train, verbose=1)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
        "    print('Training time (A2C): ', (end - start) / 60, ' minutes')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KR84ZncHeguM"
      },
      "source": [
        "def train_DDPG(env_train, model_name, timesteps=10000):\n",
        "    # add the noise objects for DDPG\n",
        "    n_actions = env_train.action_space.shape[-1]\n",
        "    param_noise = None\n",
        "    action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(0.5) * np.ones(n_actions))\n",
        "    start = time.time()\n",
        "    model = DDPG('MlpPolicy', env_train, param_noise=param_noise, action_noise=action_noise)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
        "    print('Training time (DDPG): ', (end-start)/60,' minutes')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOsQQD_pek0D"
      },
      "source": [
        "def train_PPO(env_train, model_name, timesteps=50000):  \n",
        "    start = time.time()\n",
        "    model = PPO2('MlpPolicy', env_train, ent_coef = 0.005, nminibatches = 8)\n",
        "    model.learn(total_timesteps=timesteps)\n",
        "    end = time.time()\n",
        "\n",
        "    model.save(f\"{TRAINED_MODEL_DIR}/{model_name}\")\n",
        "    print('Training time (PPO): ', (end - start) / 60, ' minutes')\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeO818RTel9F"
      },
      "source": [
        "def DRL_prediction(df,\n",
        "                   model,\n",
        "                   name,\n",
        "                   last_state,\n",
        "                   iter_num,\n",
        "                   unique_trade_date,\n",
        "                   rebalance_window,\n",
        "                   turbulence_threshold,\n",
        "                   initial):\n",
        "    ### make a prediction based on trained model### \n",
        "\n",
        "    ## trading env\n",
        "    trade_data = data_split(df, start=unique_trade_date[iter_num - rebalance_window], end=unique_trade_date[iter_num])\n",
        "    env_trade = DummyVecEnv([lambda: StockTradeEnv(trade_data,\n",
        "                                                   turbulence_threshold=turbulence_threshold,\n",
        "                                                   initial=initial,\n",
        "                                                   previous_state=last_state,\n",
        "                                                   model_name=name,\n",
        "                                                   iteration=iter_num)])\n",
        "    obs_trade = env_trade.reset()\n",
        "\n",
        "    for i in range(len(trade_data.index.unique())):\n",
        "        action, _states = model.predict(obs_trade)\n",
        "        obs_trade, rewards, dones, info = env_trade.step(action)\n",
        "        if i == (len(trade_data.index.unique()) - 2):\n",
        "            # print(env_test.render())\n",
        "            last_state = env_trade.render()\n",
        "\n",
        "    df_last_state = pd.DataFrame({'last_state': last_state})\n",
        "    df_last_state.to_csv('last_state_{}_{}.csv'.format(name, i), index=False)\n",
        "    return last_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klb1A70nezYC"
      },
      "source": [
        "def DRL_validation(model, test_data, test_env, test_obs) -> None:\n",
        "    ###validation process###\n",
        "    for i in range(len(test_data.index.unique())):\n",
        "        action, _states = model.predict(test_obs)\n",
        "        test_obs, rewards, dones, info = test_env.step(action)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH0h8SHKe3aa"
      },
      "source": [
        "def get_validation_sharpe(iteration):\n",
        "    ###Calculate Sharpe ratio based on validation results###\n",
        "    df_total_value = pd.read_csv('account_value_validation_{}.csv'.format(iteration), index_col=0)\n",
        "    df_total_value.columns = ['account_value_train']\n",
        "    df_total_value['daily_return'] = df_total_value.pct_change(1)\n",
        "    sharpe = (4 ** 0.5) * df_total_value['daily_return'].mean() / \\\n",
        "             df_total_value['daily_return'].std()\n",
        "    return sharpe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_PZK3Ctt7gB"
      },
      "source": [
        "## Running the Ensemble Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO1NJi3Ke6_d"
      },
      "source": [
        "def run_ensemble_strategy(df, unique_trade_date, rebalance_window, validation_window) -> None:\n",
        "    \"\"\"Ensemble Strategy that combines PPO, A2C and DDPG\"\"\"\n",
        "    print(\"============Start Ensemble Strategy============\")\n",
        "    # for ensemble model, it's necessary to feed the last state\n",
        "    # of the previous model to the current model as the initial state\n",
        "    last_state_ensemble = []\n",
        "\n",
        "    ppo_sharpe_list = []\n",
        "    ddpg_sharpe_list = []\n",
        "    a2c_sharpe_list = []\n",
        "\n",
        "    model_use = []\n",
        "\n",
        "    # based on the analysis of the in-sample data\n",
        "    #turbulence_threshold = 140\n",
        "    insample_turbulence = df[(df.datadate<20151000) & (df.datadate>=20090000)]\n",
        "    insample_turbulence = insample_turbulence.drop_duplicates(subset=['datadate'])\n",
        "    insample_turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, .90)\n",
        "\n",
        "    start = time.time()\n",
        "    for i in range(rebalance_window + validation_window, len(unique_trade_date), rebalance_window):\n",
        "        print(\"============================================\")\n",
        "        ## initial state is empty\n",
        "        if i - rebalance_window - validation_window == 0:\n",
        "            # inital state\n",
        "            initial = True\n",
        "        else:\n",
        "            # previous state\n",
        "            initial = False\n",
        "\n",
        "        # Tuning trubulence index based on historical data\n",
        "        # Turbulence lookback window is one quarter\n",
        "        historical_turbulence = df[(df.datadate<unique_trade_date[i - rebalance_window - validation_window]) & (df.datadate>=(unique_trade_date[i - rebalance_window - validation_window-63]))]\n",
        "        historical_turbulence = historical_turbulence.drop_duplicates(subset=['datadate'])\n",
        "        historical_turbulence_mean = np.mean(historical_turbulence.turbulence.values)   \n",
        "\n",
        "        if historical_turbulence_mean > insample_turbulence_threshold:\n",
        "            # if the mean of the historical data is greater than the 90% quantile of insample turbulence data\n",
        "            # then we assume that the current market is volatile, \n",
        "            # therefore we set the 90% quantile of insample turbulence data as the turbulence threshold \n",
        "            # meaning the current turbulence can't exceed the 90% quantile of insample turbulence data\n",
        "            turbulence_threshold = insample_turbulence_threshold\n",
        "        else:\n",
        "            # if the mean of the historical data is less than the 90% quantile of insample turbulence data\n",
        "            # then we tune up the turbulence_threshold, meaning we lower the risk \n",
        "            turbulence_threshold = np.quantile(insample_turbulence.turbulence.values, 1)\n",
        "        print(\"turbulence_threshold: \", turbulence_threshold)\n",
        "\n",
        "        ############## Environment Setup starts ##############\n",
        "        ## training env\n",
        "        train = data_split(df, start=20090000, end=unique_trade_date[i - rebalance_window - validation_window])\n",
        "        env_train = DummyVecEnv([lambda: StockTrainEnv(train)])\n",
        "\n",
        "        ## validation env\n",
        "        validation = data_split(df, start=unique_trade_date[i - rebalance_window - validation_window],\n",
        "                                end=unique_trade_date[i - rebalance_window])\n",
        "        env_val = DummyVecEnv([lambda: StockValidationEnv(validation,\n",
        "                                                          turbulence_threshold=turbulence_threshold,\n",
        "                                                          iteration=i)])\n",
        "        obs_val = env_val.reset()\n",
        "        ############## Environment Setup ends ##############\n",
        "\n",
        "        ############## Training and Validation starts ##############\n",
        "        print(\"======Model training from: \", 20090000, \"to \",\n",
        "              unique_trade_date[i - rebalance_window - validation_window])\n",
        "        # print(\"training: \",len(data_split(df, start=20090000, end=test.datadate.unique()[i-rebalance_window]) ))\n",
        "        # print(\"==============Model Training===========\")\n",
        "        print(\"======A2C Training========\")\n",
        "        model_a2c = train_A2C(env_train, model_name=\"A2C_30k_dow_{}\".format(i), timesteps=30000)\n",
        "        print(\"======A2C Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
        "              unique_trade_date[i - rebalance_window])\n",
        "        DRL_validation(model=model_a2c, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
        "        sharpe_a2c = get_validation_sharpe(i)\n",
        "        print(\"A2C Sharpe Ratio: \", sharpe_a2c)\n",
        "\n",
        "        print(\"======PPO Training========\")\n",
        "        model_ppo = train_PPO(env_train, model_name=\"PPO_100k_dow_{}\".format(i), timesteps=100000)\n",
        "        print(\"======PPO Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
        "              unique_trade_date[i - rebalance_window])\n",
        "        DRL_validation(model=model_ppo, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
        "        sharpe_ppo = get_validation_sharpe(i)\n",
        "        print(\"PPO Sharpe Ratio: \", sharpe_ppo)\n",
        "\n",
        "        print(\"======DDPG Training========\")\n",
        "        model_ddpg = train_DDPG(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=10000)\n",
        "        #model_ddpg = train_TD3(env_train, model_name=\"DDPG_10k_dow_{}\".format(i), timesteps=20000)\n",
        "        print(\"======DDPG Validation from: \", unique_trade_date[i - rebalance_window - validation_window], \"to \",\n",
        "              unique_trade_date[i - rebalance_window])\n",
        "        DRL_validation(model=model_ddpg, test_data=validation, test_env=env_val, test_obs=obs_val)\n",
        "        sharpe_ddpg = get_validation_sharpe(i)\n",
        "\n",
        "        ppo_sharpe_list.append(sharpe_ppo)\n",
        "        a2c_sharpe_list.append(sharpe_a2c)\n",
        "        ddpg_sharpe_list.append(sharpe_ddpg)\n",
        "\n",
        "        # Model Selection based on sharpe ratio\n",
        "        if (sharpe_ppo >= sharpe_a2c) & (sharpe_ppo >= sharpe_ddpg):\n",
        "            model_ensemble = model_ppo\n",
        "            model_use.append('PPO')\n",
        "        elif (sharpe_a2c > sharpe_ppo) & (sharpe_a2c > sharpe_ddpg):\n",
        "            model_ensemble = model_a2c\n",
        "            model_use.append('A2C')\n",
        "        else:\n",
        "            model_ensemble = model_ddpg\n",
        "            model_use.append('DDPG')\n",
        "        ############## Training and Validation ends ##############    \n",
        "\n",
        "        ############## Trading starts ##############    \n",
        "        print(\"======Trading from: \", unique_trade_date[i - rebalance_window], \"to \", unique_trade_date[i])\n",
        "        #print(\"Used Model: \", model_ensemble)\n",
        "        last_state_ensemble = DRL_prediction(df=df, model=model_ensemble, name=\"ensemble\",\n",
        "                                             last_state=last_state_ensemble, iter_num=i,\n",
        "                                             unique_trade_date=unique_trade_date,\n",
        "                                             rebalance_window=rebalance_window,\n",
        "                                             turbulence_threshold=turbulence_threshold,\n",
        "                                             initial=initial)\n",
        "        # print(\"============Trading Done============\")\n",
        "        ############## Trading ends ##############    \n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Ensemble Strategy took: \", (end - start) / 60, \" minutes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlRrFn87fHDt",
        "outputId": "30d933fe-1562-4858-e2a0-ff7c671534b4"
      },
      "source": [
        "def run_model() -> None:\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "\n",
        "    # read and preprocess data\n",
        "    data = preprocess_data()\n",
        "    data = add_turbulence(data)\n",
        "\n",
        "    # 2015/10/01 is the date that validation starts\n",
        "    # 2016/01/01 is the date that real trading starts\n",
        "    # unique_trade_date needs to start from 2015/10/01 for validation purpose\n",
        "    unique_trade_date = data[(data.datadate > 20151001)&(data.datadate <= 20200707)].datadate.unique()\n",
        "\n",
        "    # rebalance_window is the number of months to retrain the model\n",
        "    # validation_window is the number of months to validation the model and select for trading\n",
        "    rebalance_window = 63\n",
        "    validation_window = 63\n",
        "    \n",
        "    ## Ensemble Strategy\n",
        "    run_ensemble_strategy(df=data, \n",
        "                          unique_trade_date= unique_trade_date,\n",
        "                          rebalance_window = rebalance_window,\n",
        "                          validation_window=validation_window)\n",
        "\n",
        "    #_logger.info(f\"saving model version: {_version}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_model()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 8.6      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 0.935    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.198   |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 53.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 29.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 0.631    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 9.22     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 34.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0593  |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 22.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.3     |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 6.13     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0112   |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 104      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 303       |\n",
            "| nupdates           | 4400      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 22000     |\n",
            "| value_loss         | 53.6      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 26.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 107      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 2.76     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 20.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.123    |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 21.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 8.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 12.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.134    |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 65.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.181   |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 10.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -7.39    |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 1.27     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.358   |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 0.868    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 5.23     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 0.308    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0825  |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 4.41     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 13.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 0.434    |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.6589017828305563  minutes\n",
            "======A2C Validation from:  20171003 to  20180103\n",
            "A2C Sharpe Ratio:  0.45180216846185145\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.76145848830541  minutes\n",
            "======PPO Validation from:  20171003 to  20180103\n",
            "PPO Sharpe Ratio:  0.46888002694792563\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  0.9590605815251668  minutes\n",
            "======DDPG Validation from:  20171003 to  20180103\n",
            "======Trading from:  20180103 to  20180405\n",
            "previous_total_asset:1414204.5572948216\n",
            "end_total_asset:1427336.8057980079\n",
            "total_reward:13132.248503186274\n",
            "total_cost:  1894.0948656536052\n",
            "total trades:  262\n",
            "Sharpe:  0.06530350375649659\n",
            "============================================\n",
            "turbulence_threshold:  96.0803215835831\n",
            "======Model training from:  20090000 to  20180103\n",
            "======A2C Training========\n",
            "---------------------------------------\n",
            "| explained_variance      | -1.19e-07 |\n",
            "| fps                     | 19        |\n",
            "| nupdates                | 1         |\n",
            "| policy_entropy          | 42.6      |\n",
            "| reference_Q_mean        | 3.73      |\n",
            "| reference_Q_std         | 2.89      |\n",
            "| reference_action_mean   | 0.0449    |\n",
            "| reference_action_std    | 0.972     |\n",
            "| reference_actor_Q_mean  | 4.45      |\n",
            "| reference_actor_Q_std   | 2.79      |\n",
            "| rollout/Q_mean          | 2.35      |\n",
            "| rollout/actions_mean    | 0.00168   |\n",
            "| rollout/actions_std     | 0.795     |\n",
            "| rollout/episode_steps   | 2.2e+03   |\n",
            "| rollout/episodes        | 4         |\n",
            "| rollout/return          | 198       |\n",
            "| rollout/return_history  | 198       |\n",
            "| total/duration          | 56.8      |\n",
            "| total/episodes          | 4         |\n",
            "| total/epochs            | 1         |\n",
            "| total/steps             | 9998      |\n",
            "| total/steps_per_second  | 176       |\n",
            "| total_timesteps         | 5         |\n",
            "| train/loss_actor        | -4.32     |\n",
            "| train/loss_critic       | 1.59      |\n",
            "| train/param_noise_di... | 0         |\n",
            "| value_loss              | 0.13      |\n",
            "---------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0153  |\n",
            "| fps                | 260      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.507    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0344   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.59     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00965 |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 7.22     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 294      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 14.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.598    |\n",
            "| fps                | 290      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 1.44     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.221   |\n",
            "| fps                | 290      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 56.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.624    |\n",
            "| fps                | 290      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 0.749    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0613  |\n",
            "| fps                | 293      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 36.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 295      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 20.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.118   |\n",
            "| fps                | 293      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 1.58     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0231  |\n",
            "| fps                | 294      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 3        |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 295      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 23.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.3      |\n",
            "| fps                | 296      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 5.91     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 294      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 1.82     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0517   |\n",
            "| fps                | 295      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 12.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 296      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 13.1     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000477 |\n",
            "| fps                | 298       |\n",
            "| nupdates           | 1700      |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 8500      |\n",
            "| value_loss         | 11.7      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 299      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 7.98     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 299       |\n",
            "| nupdates           | 1900      |\n",
            "| policy_entropy     | 42.7      |\n",
            "| total_timesteps    | 9500      |\n",
            "| value_loss         | 2.43      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 39       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 1.23     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 42.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0942  |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 5.52     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 28.8     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -3.58e-07 |\n",
            "| fps                | 302       |\n",
            "| nupdates           | 2500      |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 12500     |\n",
            "| value_loss         | 13.7      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000588 |\n",
            "| fps                | 302       |\n",
            "| nupdates           | 2600      |\n",
            "| policy_entropy     | 42.7      |\n",
            "| total_timesteps    | 13000     |\n",
            "| value_loss         | 290       |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 3.65     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0103   |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 20.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 10.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0369   |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 84.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -2.39    |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 9.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0134  |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 19       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.127    |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 1.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 22       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 6.52     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 0.329    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0633   |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 25.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0497   |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 11.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 1.37     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 300       |\n",
            "| nupdates           | 4000      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 20000     |\n",
            "| value_loss         | 21.1      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0016  |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 1.95     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.000394 |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 2.33     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.556    |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 0.62     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.207    |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 0.289    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.383    |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 0.125    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000606 |\n",
            "| fps                | 301       |\n",
            "| nupdates           | 4600      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 23000     |\n",
            "| value_loss         | 3.92      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.977   |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 4.65     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.212    |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 2.16     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 3.4e-06  |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 1.36     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 1.21     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 5.87     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.979   |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 2.06     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 6.34e-05 |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 2.03     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0931  |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 9.32     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 302      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 1.29     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0935  |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 1.04     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0866  |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 1.65     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 2.76     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 11.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 303      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 0.771    |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.6620398044586182  minutes\n",
            "======A2C Validation from:  20180103 to  20180405\n",
            "A2C Sharpe Ratio:  0.06678048324789976\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.706482390562694  minutes\n",
            "======PPO Validation from:  20180103 to  20180405\n",
            "PPO Sharpe Ratio:  -0.036371933036175114\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.0076155424118043  minutes\n",
            "======DDPG Validation from:  20180103 to  20180405\n",
            "======Trading from:  20180405 to  20180705\n",
            "previous_total_asset:1427336.8057980079\n",
            "end_total_asset:1406818.2382560172\n",
            "total_reward:-20518.567541990662\n",
            "total_cost:  5485.670614871681\n",
            "total trades:  1079\n",
            "Sharpe:  -0.07562854997774647\n",
            "============================================\n",
            "turbulence_threshold:  96.0803215835831\n",
            "======Model training from:  20090000 to  20180405\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 19       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 4.14     |\n",
            "| reference_Q_std         | 2.95     |\n",
            "| reference_action_mean   | 0.0261   |\n",
            "| reference_action_std    | 0.97     |\n",
            "| reference_actor_Q_mean  | 4.86     |\n",
            "| reference_actor_Q_std   | 3.12     |\n",
            "| rollout/Q_mean          | 2.05     |\n",
            "| rollout/actions_mean    | 0.0336   |\n",
            "| rollout/actions_std     | 0.811    |\n",
            "| rollout/episode_steps   | 2.27e+03 |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 264      |\n",
            "| rollout/return_history  | 264      |\n",
            "| total/duration          | 59.6     |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 168      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.34    |\n",
            "| train/loss_critic       | 1.88     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.0744   |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.31    |\n",
            "| fps                | 262      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 1.37     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0776   |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.22     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0598  |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 2.59     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00764 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 9.71     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0988  |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 5.14     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.148   |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 11.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.41    |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 14.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0212   |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 188      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.127   |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 3.1      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 285       |\n",
            "| nupdates           | 1000      |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 5000      |\n",
            "| value_loss         | 9.24      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.281    |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 4.49     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 0.309    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.475    |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 3.67     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0339  |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 0.0873   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0539  |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 1.93     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0362   |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 1.81     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00301  |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 5.81     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 289       |\n",
            "| nupdates           | 1800      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 9000      |\n",
            "| value_loss         | 2.82      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0301   |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 0.789    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0876   |\n",
            "| fps                | 290      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 4.01     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.122    |\n",
            "| fps                | 291      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 8.09     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 292      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 1.61     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.208   |\n",
            "| fps                | 293      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 16.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0392  |\n",
            "| fps                | 292      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 4.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.206   |\n",
            "| fps                | 293      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 0.482    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.269   |\n",
            "| fps                | 294      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 0.856    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0679  |\n",
            "| fps                | 294      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 16.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 294      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 1.57     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0816   |\n",
            "| fps                | 293      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 0.407    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.224    |\n",
            "| fps                | 294      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 1.43     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.131   |\n",
            "| fps                | 295      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 12.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0613   |\n",
            "| fps                | 295      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 0.173    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.4     |\n",
            "| fps                | 295      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 2.4      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0313  |\n",
            "| fps                | 295      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 8.66     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.1      |\n",
            "| fps                | 296      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 1.77     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 296      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 13.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 297      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 1.44     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 296       |\n",
            "| nupdates           | 3800      |\n",
            "| policy_entropy     | 43.4      |\n",
            "| total_timesteps    | 19000     |\n",
            "| value_loss         | 0.67      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.329   |\n",
            "| fps                | 297      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 1.1      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0988  |\n",
            "| fps                | 297      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 0.621    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.18    |\n",
            "| fps                | 297      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 2.19     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 2.38e-07 |\n",
            "| fps                | 297      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 11.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.178    |\n",
            "| fps                | 297      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 4.31     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 297      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 0.621    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.261   |\n",
            "| fps                | 297      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 2.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.249    |\n",
            "| fps                | 298      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 0.397    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.161   |\n",
            "| fps                | 298      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 1.57     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.403   |\n",
            "| fps                | 298      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 8.08     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0554  |\n",
            "| fps                | 298      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 3.24     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0947   |\n",
            "| fps                | 298      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 10.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 299      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 0.348    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.246    |\n",
            "| fps                | 299      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 14       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 299      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 9.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 14       |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 300       |\n",
            "| nupdates           | 5500      |\n",
            "| policy_entropy     | 43.4      |\n",
            "| total_timesteps    | 27500     |\n",
            "| value_loss         | 1.87      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.125    |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 6.6      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0309   |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 6.69     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.187   |\n",
            "| fps                | 300      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 0.605    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.6     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 7.7      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 301      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.6     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 0.679    |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.674096655845642  minutes\n",
            "======A2C Validation from:  20180405 to  20180705\n",
            "A2C Sharpe Ratio:  -0.017978553968084936\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.925338598092397  minutes\n",
            "======PPO Validation from:  20180405 to  20180705\n",
            "PPO Sharpe Ratio:  -0.11572336479873155\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.082551944255829  minutes\n",
            "======DDPG Validation from:  20180405 to  20180705\n",
            "======Trading from:  20180705 to  20181003\n",
            "previous_total_asset:1406818.2382560172\n",
            "end_total_asset:1427407.598667263\n",
            "total_reward:20589.36041124584\n",
            "total_cost:  5016.465813809485\n",
            "total trades:  624\n",
            "Sharpe:  0.17946698225260646\n",
            "============================================\n",
            "turbulence_threshold:  96.0803215835831\n",
            "======Model training from:  20090000 to  20180705\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 17       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 3.4      |\n",
            "| reference_Q_std         | 2.73     |\n",
            "| reference_action_mean   | -0.0707  |\n",
            "| reference_action_std    | 0.957    |\n",
            "| reference_actor_Q_mean  | 4.17     |\n",
            "| reference_actor_Q_std   | 2.82     |\n",
            "| rollout/Q_mean          | 2.12     |\n",
            "| rollout/actions_mean    | -0.0201  |\n",
            "| rollout/actions_std     | 0.819    |\n",
            "| rollout/episode_steps   | 2.33e+03 |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 119      |\n",
            "| rollout/return_history  | 119      |\n",
            "| total/duration          | 64.1     |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 156      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.11    |\n",
            "| train/loss_critic       | 1.49     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.0912   |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.18    |\n",
            "| fps                | 242      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.625    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0181   |\n",
            "| fps                | 259      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.62     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0278  |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 7.19     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.013    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 9.01     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 248      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 1.09     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.358   |\n",
            "| fps                | 253      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 3.82     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.184    |\n",
            "| fps                | 256      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 0.968    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0571   |\n",
            "| fps                | 257      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 2.36     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.134    |\n",
            "| fps                | 260      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 36.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00737 |\n",
            "| fps                | 260      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 22.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.035   |\n",
            "| fps                | 261      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 9.1      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0447  |\n",
            "| fps                | 263      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 1.19     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0143   |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 18.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0245   |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 55       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.965   |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 2.41     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0535  |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 10.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.138    |\n",
            "| fps                | 259      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 55       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 260      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 48.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00423  |\n",
            "| fps                | 259      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 341      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0175   |\n",
            "| fps                | 258      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 7.74     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 259       |\n",
            "| nupdates           | 2100      |\n",
            "| policy_entropy     | 42.2      |\n",
            "| total_timesteps    | 10500     |\n",
            "| value_loss         | 12.9      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 261      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 1.84     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 262      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 49       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 262      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 6.97     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 263      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 7.62     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 264      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 10.6     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 265       |\n",
            "| nupdates           | 2700      |\n",
            "| policy_entropy     | 42.2      |\n",
            "| total_timesteps    | 13500     |\n",
            "| value_loss         | 156       |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0126   |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 38.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00475 |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 11.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 268      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 14.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.469   |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 0.838    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 51.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 6.98     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0037   |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 1.88     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 1.72     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 2.93     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 268      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 24.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.34    |\n",
            "| fps                | 268      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 31       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.114    |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 12.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.139   |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 15       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.45    |\n",
            "| fps                | 268      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 7.35     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0552  |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 6.79     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 17.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 2.38e-07 |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 44       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.398   |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 18.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 10.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.24    |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 2.48     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00678  |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 14.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.124    |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 64.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.126   |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 42.1     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 11.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0813   |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 30.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.132    |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 0.281    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00243 |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 6.16     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0975   |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 18.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.108    |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 42.2     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 11.3     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -4.77e-07 |\n",
            "| fps                | 265       |\n",
            "| nupdates           | 5600      |\n",
            "| policy_entropy     | 42.3      |\n",
            "| total_timesteps    | 28000     |\n",
            "| value_loss         | 14.8      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 18.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0519  |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 4.3      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 0.265    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 31.5     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.89174329439799  minutes\n",
            "======A2C Validation from:  20180705 to  20181003\n",
            "A2C Sharpe Ratio:  0.1235413897094453\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.965597240130107  minutes\n",
            "======PPO Validation from:  20180705 to  20181003\n",
            "PPO Sharpe Ratio:  0.003751020370896603\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.022601310412089  minutes\n",
            "======DDPG Validation from:  20180705 to  20181003\n",
            "======Trading from:  20181003 to  20190104\n",
            "previous_total_asset:1427407.598667263\n",
            "end_total_asset:1435020.7410475276\n",
            "total_reward:7613.142380264588\n",
            "total_cost:  793.6189799262893\n",
            "total trades:  140\n",
            "Sharpe:  0.22204255757421043\n",
            "============================================\n",
            "turbulence_threshold:  171.09407156310454\n",
            "======Model training from:  20090000 to  20181003\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 19       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 3.91     |\n",
            "| reference_Q_std         | 3.83     |\n",
            "| reference_action_mean   | -0.0773  |\n",
            "| reference_action_std    | 0.958    |\n",
            "| reference_actor_Q_mean  | 4.92     |\n",
            "| reference_actor_Q_std   | 3.7      |\n",
            "| rollout/Q_mean          | 2.64     |\n",
            "| rollout/actions_mean    | -0.105   |\n",
            "| rollout/actions_std     | 0.798    |\n",
            "| rollout/episode_steps   | 2.39e+03 |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 304      |\n",
            "| rollout/return_history  | 304      |\n",
            "| total/duration          | 60.6     |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 165      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.73    |\n",
            "| train/loss_critic       | 2.43     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.0704   |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.988   |\n",
            "| fps                | 250      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 1.96     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00563 |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 268      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 5.3      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0321  |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 23       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 272      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 0.863    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.313    |\n",
            "| fps                | 275      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 6.26     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.128   |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 3.83     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.332   |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 1.2      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0627  |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 3.01     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 2.98e-07 |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 2.7      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0648  |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 1.06     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.738   |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 2.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.163   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 1.89     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00747 |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 3.07     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.183   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 7.15     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.179   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 3.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00458 |\n",
            "| fps                | 276      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 13.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0594   |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 3.97     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.848    |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 1.16     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.352   |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 0.248    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00598 |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 4.72     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0009   |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 25.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 17.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.34    |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 16.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.053   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 1.64     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0847  |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 16.5     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 281       |\n",
            "| nupdates           | 2700      |\n",
            "| policy_entropy     | 42.4      |\n",
            "| total_timesteps    | 13500     |\n",
            "| value_loss         | 1.08      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 282       |\n",
            "| nupdates           | 2800      |\n",
            "| policy_entropy     | 42.4      |\n",
            "| total_timesteps    | 14000     |\n",
            "| value_loss         | 15.3      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 2.22     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.248   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 3.02     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00538 |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 0.57     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00142 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 0.315    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.867   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 7.75     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.139    |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 0.737    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0226   |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 1.42     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.74e-06 |\n",
            "| fps                | 282       |\n",
            "| nupdates           | 3600      |\n",
            "| policy_entropy     | 42.3      |\n",
            "| total_timesteps    | 18000     |\n",
            "| value_loss         | 2.67      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 1.48     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0855   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 48.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 54       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0511   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 14       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0736   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 38.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 0.437    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0497  |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 4.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.301   |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 42.3     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 15.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 13.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 3.33     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0464  |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 35.4     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 285       |\n",
            "| nupdates           | 4800      |\n",
            "| policy_entropy     | 42.4      |\n",
            "| total_timesteps    | 24000     |\n",
            "| value_loss         | 24.8      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 42.4     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 4.35     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 0.295    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.737    |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 0.124    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.211    |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 1.32     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.214   |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 29       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 2.41     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 0.303    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 3.15     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 79.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.107   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 2.78     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 18.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0229  |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 2.57     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.7785856445630392  minutes\n",
            "======A2C Validation from:  20181003 to  20190104\n",
            "A2C Sharpe Ratio:  -0.32526832811909523\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.901509952545166  minutes\n",
            "======PPO Validation from:  20181003 to  20190104\n",
            "PPO Sharpe Ratio:  -0.40723765140927765\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.082925021648407  minutes\n",
            "======DDPG Validation from:  20181003 to  20190104\n",
            "======Trading from:  20190104 to  20190405\n",
            "previous_total_asset:1435020.7410475276\n",
            "end_total_asset:1548865.3736018478\n",
            "total_reward:113844.63255432015\n",
            "total_cost:  6921.096392271133\n",
            "total trades:  1309\n",
            "Sharpe:  0.37392124443642344\n",
            "============================================\n",
            "turbulence_threshold:  96.0803215835831\n",
            "======Model training from:  20090000 to  20190104\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 17       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 4.31     |\n",
            "| reference_Q_std         | 2.65     |\n",
            "| reference_action_mean   | 0.097    |\n",
            "| reference_action_std    | 0.971    |\n",
            "| reference_actor_Q_mean  | 5.19     |\n",
            "| reference_actor_Q_std   | 2.65     |\n",
            "| rollout/Q_mean          | 2.84     |\n",
            "| rollout/actions_mean    | -0.0306  |\n",
            "| rollout/actions_std     | 0.818    |\n",
            "| rollout/episode_steps   | 2.46e+03 |\n",
            "| rollout/episodes        | 4        |\n",
            "| rollout/return          | 314      |\n",
            "| rollout/return_history  | 314      |\n",
            "| total/duration          | 64.2     |\n",
            "| total/episodes          | 4        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 156      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -5.35    |\n",
            "| train/loss_critic       | 1.93     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.0289   |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.685    |\n",
            "| fps                | 253      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.444    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 264      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.56     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.164   |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 5.73     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.194   |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 23       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 275      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 95.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.221   |\n",
            "| fps                | 255      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 4.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.101   |\n",
            "| fps                | 258      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 1.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.408   |\n",
            "| fps                | 261      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 0.808    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.468    |\n",
            "| fps                | 264      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 6.02     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.11     |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 11       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.136    |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.5     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 13.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0492  |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 1.79     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0651  |\n",
            "| fps                | 272      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 17.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.51    |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 1.02     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00925 |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 17.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0546   |\n",
            "| fps                | 275      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 1.76     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0223   |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 0.461    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.391   |\n",
            "| fps                | 276      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 16       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 9.12e-06 |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 5.43     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0357   |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 38.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00737 |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 7.25     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0502   |\n",
            "| fps                | 275      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 40.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.192   |\n",
            "| fps                | 276      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 0.132    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.533   |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 0.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.147    |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 0.891    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.678   |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 2.5      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.881   |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 0.498    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0573  |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 2.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0689   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 6.86     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 20.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.158   |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 7.42     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 276       |\n",
            "| nupdates           | 3200      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 16000     |\n",
            "| value_loss         | 7.97      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 4.73     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 1.6      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0524   |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 3.66     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 7.98     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.185   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 1.72     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 0.52     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0216   |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 0.0972   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00587  |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 11.6     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 282       |\n",
            "| nupdates           | 4100      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 20500     |\n",
            "| value_loss         | 6         |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00539  |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 0.451    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0182   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 1.95     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.12    |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 3.3      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 46.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0152  |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 51.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00568 |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 0.605    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 1.98     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0028  |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 18.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00203  |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 9.98     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00548 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 0.489    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.000711 |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 0.873    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 16.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 3.78e-05 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 8.58     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00342 |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 64.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0418  |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 0.867    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 1.77     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0088  |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 0.885    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 15.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 14.8     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.7621680657068888  minutes\n",
            "======A2C Validation from:  20190104 to  20190405\n",
            "A2C Sharpe Ratio:  -0.02113564698048344\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.927154409885406  minutes\n",
            "======PPO Validation from:  20190104 to  20190405\n",
            "PPO Sharpe Ratio:  -0.04252355411480757\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.0354586164156596  minutes\n",
            "======DDPG Validation from:  20190104 to  20190405\n",
            "======Trading from:  20190405 to  20190708\n",
            "previous_total_asset:1548865.3736018478\n",
            "end_total_asset:1552337.3637830452\n",
            "total_reward:3471.9901811974123\n",
            "total_cost:  1216.4714395582525\n",
            "total trades:  127\n",
            "Sharpe:  0.19099442692420743\n",
            "============================================\n",
            "turbulence_threshold:  96.0803215835831\n",
            "======Model training from:  20090000 to  20190405\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 16       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 3.14     |\n",
            "| reference_Q_std         | 2.92     |\n",
            "| reference_action_mean   | 0.0413   |\n",
            "| reference_action_std    | 0.963    |\n",
            "| reference_actor_Q_mean  | 4.68     |\n",
            "| reference_actor_Q_std   | 2.97     |\n",
            "| rollout/Q_mean          | 2        |\n",
            "| rollout/actions_mean    | 0.0679   |\n",
            "| rollout/actions_std     | 0.813    |\n",
            "| rollout/episode_steps   | 2.52e+03 |\n",
            "| rollout/episodes        | 3        |\n",
            "| rollout/return          | 271      |\n",
            "| rollout/return_history  | 271      |\n",
            "| total/duration          | 61.4     |\n",
            "| total/episodes          | 3        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 163      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.92    |\n",
            "| train/loss_critic       | 2.85     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.178    |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.86    |\n",
            "| fps                | 216      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 2.98     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.31     |\n",
            "| fps                | 234      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.93     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.544    |\n",
            "| fps                | 246      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 4.79     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.72    |\n",
            "| fps                | 250      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 37.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.177    |\n",
            "| fps                | 252      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 74.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.111   |\n",
            "| fps                | 252      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 7.02     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.228    |\n",
            "| fps                | 256      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 0.164    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.268   |\n",
            "| fps                | 259      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 3.9      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.403    |\n",
            "| fps                | 262      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 1.37     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.05    |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 9.07     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.107    |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 80.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0684   |\n",
            "| fps                | 268      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 1.77     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.19    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 2.35     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0166   |\n",
            "| fps                | 272      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 3.38     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 67.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00608 |\n",
            "| fps                | 263      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 0.981    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0135  |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 1.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.384    |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 3.59     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0798  |\n",
            "| fps                | 268      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 22.1     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 270       |\n",
            "| nupdates           | 2000      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 10000     |\n",
            "| value_loss         | 1.7       |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0318  |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 5.86     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00603 |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 6.06     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.115    |\n",
            "| fps                | 271      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 23.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0101   |\n",
            "| fps                | 272      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 63.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0715   |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 1.99     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.594   |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 3.9      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 275      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 1.64     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 276      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 4.81     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 18.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 18.9     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 278       |\n",
            "| nupdates           | 3100      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 15500     |\n",
            "| value_loss         | 0.39      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 2.61     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 277       |\n",
            "| nupdates           | 3300      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 16500     |\n",
            "| value_loss         | 1.97      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 28.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00185 |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 3.39     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 279       |\n",
            "| nupdates           | 3600      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 18000     |\n",
            "| value_loss         | 34.2      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.381    |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 3.2      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 2.32     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 8.64     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 4.05     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 283       |\n",
            "| nupdates           | 4100      |\n",
            "| policy_entropy     | 43.2      |\n",
            "| total_timesteps    | 20500     |\n",
            "| value_loss         | 6.29      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 5.83     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 8.74     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 3.32     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 283       |\n",
            "| nupdates           | 4500      |\n",
            "| policy_entropy     | 43.4      |\n",
            "| total_timesteps    | 22500     |\n",
            "| value_loss         | 4.74      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 39       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0646   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 1.45     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.436    |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 0.0619   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.32    |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 0.331    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0454   |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 6.68     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00353  |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 0.96     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 8.94     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0639   |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 8.93     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 4.89     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0282  |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 5.64     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.184   |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 34.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0186  |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 0.658    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0134  |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 11.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.166    |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 34.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 6.51     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.7408030907313028  minutes\n",
            "======A2C Validation from:  20190405 to  20190708\n",
            "A2C Sharpe Ratio:  0.09295783392934848\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.942688318093618  minutes\n",
            "======PPO Validation from:  20190405 to  20190708\n",
            "PPO Sharpe Ratio:  0.24644432654766\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.0393129189809163  minutes\n",
            "======DDPG Validation from:  20190405 to  20190708\n",
            "======Trading from:  20190708 to  20191004\n",
            "previous_total_asset:1552337.3637830452\n",
            "end_total_asset:1560058.1156736277\n",
            "total_reward:7720.751890582498\n",
            "total_cost:  1843.9530329509646\n",
            "total trades:  321\n",
            "Sharpe:  0.14203047462258395\n",
            "============================================\n",
            "turbulence_threshold:  96.0803215835831\n",
            "======Model training from:  20090000 to  20190708\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 18       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 3.03     |\n",
            "| reference_Q_std         | 2.91     |\n",
            "| reference_action_mean   | -0.246   |\n",
            "| reference_action_std    | 0.94     |\n",
            "| reference_actor_Q_mean  | 3.89     |\n",
            "| reference_actor_Q_std   | 2.87     |\n",
            "| rollout/Q_mean          | 2.16     |\n",
            "| rollout/actions_mean    | -0.15    |\n",
            "| rollout/actions_std     | 0.79     |\n",
            "| rollout/episode_steps   | 2.58e+03 |\n",
            "| rollout/episodes        | 3        |\n",
            "| rollout/return          | 119      |\n",
            "| rollout/return_history  | 119      |\n",
            "| total/duration          | 61.5     |\n",
            "| total/episodes          | 3        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 162      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -4.17    |\n",
            "| train/loss_critic       | 1.27     |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.139    |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.463    |\n",
            "| fps                | 243      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.635    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0545   |\n",
            "| fps                | 266      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 0.832    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0645  |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 5.19     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0581   |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 13.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.246    |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 109      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.386   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 2.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.199   |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 0.589    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00842 |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 1.68     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.108   |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 3.71     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0598   |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 11.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.154    |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 1.04     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0783  |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 13.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 7.2      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0634   |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 3.6      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 0.803    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 287       |\n",
            "| nupdates           | 1600      |\n",
            "| policy_entropy     | 42.7      |\n",
            "| total_timesteps    | 8000      |\n",
            "| value_loss         | 2.61      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.416   |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 0.0759   |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.16    |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 1800     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 9000     |\n",
            "| value_loss         | 1.91     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 19.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0176   |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 14.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0123   |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 16.7     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.23e-05 |\n",
            "| fps                | 284       |\n",
            "| nupdates           | 2200      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 11000     |\n",
            "| value_loss         | 2.26      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 0.373    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 4.4      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.194    |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 8.96     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0978  |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 22.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.373    |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 1.55     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.39     |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 0.217    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 19.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0124   |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 25.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.493    |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 1.18     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.182    |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 13.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0229  |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 8.46     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.278   |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 12.9     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 285       |\n",
            "| nupdates           | 3500      |\n",
            "| policy_entropy     | 43.1      |\n",
            "| total_timesteps    | 17500     |\n",
            "| value_loss         | 6.58      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 2.38e-07 |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 3600     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 18000    |\n",
            "| value_loss         | 3.14     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.248    |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 1.83     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00393 |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 1.51     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0416  |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 5.89     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.32     |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 1.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 2.64     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 32.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0427   |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 36.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0441   |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 11.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.38    |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 5.16     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.000597 |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 9.89     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0245   |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 52.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00258 |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 4.98     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0427   |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 1.66     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.186   |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 12.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.139    |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 9.42     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 18.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.181   |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 8.91     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0415   |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 5400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 27000    |\n",
            "| value_loss         | 1.33     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 288      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 4.44     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00915  |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 2.17     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.95    |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 4.49     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 290      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 29.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.572    |\n",
            "| fps                | 289      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 0.811    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.59    |\n",
            "| fps                | 290      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 11.5     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.7343742370605468  minutes\n",
            "======A2C Validation from:  20190708 to  20191004\n",
            "A2C Sharpe Ratio:  -0.015313634706005948\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.8309667110443115  minutes\n",
            "======PPO Validation from:  20190708 to  20191004\n",
            "PPO Sharpe Ratio:  -0.13196520184179222\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.0181052327156066  minutes\n",
            "======DDPG Validation from:  20190708 to  20191004\n",
            "======Trading from:  20191004 to  20200106\n",
            "previous_total_asset:1560058.1156736277\n",
            "end_total_asset:1559235.8899811544\n",
            "total_reward:-822.225692473352\n",
            "total_cost:  365.11870073284143\n",
            "total trades:  72\n",
            "Sharpe:  -0.23338596936505365\n",
            "============================================\n",
            "turbulence_threshold:  96.0803215835831\n",
            "======Model training from:  20090000 to  20191004\n",
            "======A2C Training========\n",
            "---------------------------------------\n",
            "| explained_variance      | -2.38e-07 |\n",
            "| fps                     | 17        |\n",
            "| nupdates                | 1         |\n",
            "| policy_entropy          | 42.6      |\n",
            "| reference_Q_mean        | 4.02      |\n",
            "| reference_Q_std         | 3.9       |\n",
            "| reference_action_mean   | 0.0975    |\n",
            "| reference_action_std    | 0.961     |\n",
            "| reference_actor_Q_mean  | 5.2       |\n",
            "| reference_actor_Q_std   | 3.95      |\n",
            "| rollout/Q_mean          | 2.08      |\n",
            "| rollout/actions_mean    | 0.0646    |\n",
            "| rollout/actions_std     | 0.82      |\n",
            "| rollout/episode_steps   | 2.64e+03  |\n",
            "| rollout/episodes        | 3         |\n",
            "| rollout/return          | 391       |\n",
            "| rollout/return_history  | 391       |\n",
            "| total/duration          | 60.3      |\n",
            "| total/episodes          | 3         |\n",
            "| total/epochs            | 1         |\n",
            "| total/steps             | 9998      |\n",
            "| total/steps_per_second  | 166       |\n",
            "| total_timesteps         | 5         |\n",
            "| train/loss_actor        | -4.93     |\n",
            "| train/loss_critic       | 4.91      |\n",
            "| train/param_noise_di... | 0         |\n",
            "| value_loss              | 0.0839    |\n",
            "---------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.158    |\n",
            "| fps                | 236      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.249    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.257    |\n",
            "| fps                | 256      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.87     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0203  |\n",
            "| fps                | 264      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 4.8      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.15    |\n",
            "| fps                | 265      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 14.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00877 |\n",
            "| fps                | 267      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 119      |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.91e-06 |\n",
            "| fps                | 267       |\n",
            "| nupdates           | 600       |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 3000      |\n",
            "| value_loss         | 2.13      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.11    |\n",
            "| fps                | 270      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 1.48     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 272      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 2.05     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0444  |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 41.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0608  |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 53.1     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 274       |\n",
            "| nupdates           | 1100      |\n",
            "| policy_entropy     | 42.7      |\n",
            "| total_timesteps    | 5500      |\n",
            "| value_loss         | 0.934     |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.185    |\n",
            "| fps                | 272      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 2.27     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.163   |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 2.76     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 11.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 275      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 7.76     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0172   |\n",
            "| fps                | 276      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 7.72     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.158   |\n",
            "| fps                | 276      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 9.58     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 277       |\n",
            "| nupdates           | 1800      |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 9000      |\n",
            "| value_loss         | 2.26      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 6.38     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -3.58e-07 |\n",
            "| fps                | 279       |\n",
            "| nupdates           | 2000      |\n",
            "| policy_entropy     | 42.6      |\n",
            "| total_timesteps    | 10000     |\n",
            "| value_loss         | 1.91      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0794  |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 22.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0323   |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 22.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 3.36     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0255  |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 2.94     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0211  |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2500     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 12500    |\n",
            "| value_loss         | 403      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0322  |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 16.3     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -8.73e-05 |\n",
            "| fps                | 280       |\n",
            "| nupdates           | 2700      |\n",
            "| policy_entropy     | 42.7      |\n",
            "| total_timesteps    | 13500     |\n",
            "| value_loss         | 35.7      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.032   |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 29.9     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.383   |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 8.03     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 3000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 15000    |\n",
            "| value_loss         | 11.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.251    |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 17.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00713  |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 76.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.159   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 0.956    |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 280       |\n",
            "| nupdates           | 3400      |\n",
            "| policy_entropy     | 42.8      |\n",
            "| total_timesteps    | 17000     |\n",
            "| value_loss         | 15.7      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0128   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 1.97     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 281       |\n",
            "| nupdates           | 3600      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 18000     |\n",
            "| value_loss         | 16.2      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.175    |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 7.68     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 6.42     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0859  |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 2.24     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 5.84     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0217  |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 3.25     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 4.54     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0389  |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 7.34     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0476   |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 4400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 22000    |\n",
            "| value_loss         | 27.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.119    |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 17.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0019  |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 2.16     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.31e-06 |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 74.6     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0264  |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 72.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0334   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 68.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 27.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 9.11     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 281       |\n",
            "| nupdates           | 5200      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 26000     |\n",
            "| value_loss         | 46.1      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 15.4     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -6.03e-05 |\n",
            "| fps                | 281       |\n",
            "| nupdates           | 5400      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 27000     |\n",
            "| value_loss         | 45.1      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 5500     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 27500    |\n",
            "| value_loss         | 3.82     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 281       |\n",
            "| nupdates           | 5600      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 28000     |\n",
            "| value_loss         | 16        |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.727   |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 5.63     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.311    |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 2.12     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.34    |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 53.5     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0196   |\n",
            "| fps                | 282      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 5.76     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.7789831240971883  minutes\n",
            "======A2C Validation from:  20191004 to  20200106\n",
            "A2C Sharpe Ratio:  -0.37629467807656436\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.75714062054952  minutes\n",
            "======PPO Validation from:  20191004 to  20200106\n",
            "PPO Sharpe Ratio:  0.06970548023941435\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.0631249308586121  minutes\n",
            "======DDPG Validation from:  20191004 to  20200106\n",
            "======Trading from:  20200106 to  20200406\n",
            "previous_total_asset:1559235.8899811544\n",
            "end_total_asset:1542687.4396004216\n",
            "total_reward:-16548.45038073277\n",
            "total_cost:  895.1136839126539\n",
            "total trades:  183\n",
            "Sharpe:  -0.41369775784879964\n",
            "============================================\n",
            "turbulence_threshold:  96.0803215835831\n",
            "======Model training from:  20090000 to  20200106\n",
            "======A2C Training========\n",
            "--------------------------------------\n",
            "| explained_variance      | 0        |\n",
            "| fps                     | 18       |\n",
            "| nupdates                | 1        |\n",
            "| policy_entropy          | 42.6     |\n",
            "| reference_Q_mean        | 3.48     |\n",
            "| reference_Q_std         | 3.63     |\n",
            "| reference_action_mean   | 0.0529   |\n",
            "| reference_action_std    | 0.974    |\n",
            "| reference_actor_Q_mean  | 4.71     |\n",
            "| reference_actor_Q_std   | 3.72     |\n",
            "| rollout/Q_mean          | 2.79     |\n",
            "| rollout/actions_mean    | 0.0585   |\n",
            "| rollout/actions_std     | 0.806    |\n",
            "| rollout/episode_steps   | 2.71e+03 |\n",
            "| rollout/episodes        | 3        |\n",
            "| rollout/return          | 298      |\n",
            "| rollout/return_history  | 298      |\n",
            "| total/duration          | 63       |\n",
            "| total/episodes          | 3        |\n",
            "| total/epochs            | 1        |\n",
            "| total/steps             | 9998     |\n",
            "| total/steps_per_second  | 159      |\n",
            "| total_timesteps         | 5        |\n",
            "| train/loss_actor        | -5.37    |\n",
            "| train/loss_critic       | 2.4      |\n",
            "| train/param_noise_di... | 0        |\n",
            "| value_loss              | 0.173    |\n",
            "--------------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.321    |\n",
            "| fps                | 254      |\n",
            "| nupdates           | 100      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 500      |\n",
            "| value_loss         | 0.306    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0183  |\n",
            "| fps                | 269      |\n",
            "| nupdates           | 200      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1000     |\n",
            "| value_loss         | 1.4      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.148   |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 300      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 1500     |\n",
            "| value_loss         | 8.66     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.133   |\n",
            "| fps                | 277      |\n",
            "| nupdates           | 400      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2000     |\n",
            "| value_loss         | 18.2     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0443  |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 500      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 2500     |\n",
            "| value_loss         | 175      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0848   |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 600      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3000     |\n",
            "| value_loss         | 0.855    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0415  |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 700      |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 3500     |\n",
            "| value_loss         | 40       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.759   |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 800      |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 4000     |\n",
            "| value_loss         | 0.682    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 900      |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 4500     |\n",
            "| value_loss         | 12.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -1.51    |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 1000     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 5000     |\n",
            "| value_loss         | 0.854    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00574  |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 1100     |\n",
            "| policy_entropy     | 42.6     |\n",
            "| total_timesteps    | 5500     |\n",
            "| value_loss         | 17.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -3.32    |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 1200     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6000     |\n",
            "| value_loss         | 2.39     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -2.19    |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 1300     |\n",
            "| policy_entropy     | 42.7     |\n",
            "| total_timesteps    | 6500     |\n",
            "| value_loss         | 6.52     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0329  |\n",
            "| fps                | 273      |\n",
            "| nupdates           | 1400     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 7000     |\n",
            "| value_loss         | 10.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0415   |\n",
            "| fps                | 274      |\n",
            "| nupdates           | 1500     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 7500     |\n",
            "| value_loss         | 1.84     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 275      |\n",
            "| nupdates           | 1600     |\n",
            "| policy_entropy     | 42.8     |\n",
            "| total_timesteps    | 8000     |\n",
            "| value_loss         | 325      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.136   |\n",
            "| fps                | 276      |\n",
            "| nupdates           | 1700     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 8500     |\n",
            "| value_loss         | 5.05     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000246 |\n",
            "| fps                | 277       |\n",
            "| nupdates           | 1800      |\n",
            "| policy_entropy     | 42.9      |\n",
            "| total_timesteps    | 9000      |\n",
            "| value_loss         | 9.56      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 1900     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 9500     |\n",
            "| value_loss         | 12.1     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.000984 |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2000     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 10000    |\n",
            "| value_loss         | 5.2      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0943  |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 2100     |\n",
            "| policy_entropy     | 42.9     |\n",
            "| total_timesteps    | 10500    |\n",
            "| value_loss         | 0.452    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.111   |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 2200     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 11000    |\n",
            "| value_loss         | 3.28     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0131   |\n",
            "| fps                | 278      |\n",
            "| nupdates           | 2300     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 11500    |\n",
            "| value_loss         | 13.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.273    |\n",
            "| fps                | 279      |\n",
            "| nupdates           | 2400     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 12000    |\n",
            "| value_loss         | 4.09     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000149 |\n",
            "| fps                | 279       |\n",
            "| nupdates           | 2500      |\n",
            "| policy_entropy     | 43        |\n",
            "| total_timesteps    | 12500     |\n",
            "| value_loss         | 1.94      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.43    |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2600     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 13000    |\n",
            "| value_loss         | 1.34     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2700     |\n",
            "| policy_entropy     | 43       |\n",
            "| total_timesteps    | 13500    |\n",
            "| value_loss         | 4.11     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.475   |\n",
            "| fps                | 280      |\n",
            "| nupdates           | 2800     |\n",
            "| policy_entropy     | 43.1     |\n",
            "| total_timesteps    | 14000    |\n",
            "| value_loss         | 0.75     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0232  |\n",
            "| fps                | 281      |\n",
            "| nupdates           | 2900     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 14500    |\n",
            "| value_loss         | 72       |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.43e-06 |\n",
            "| fps                | 282       |\n",
            "| nupdates           | 3000      |\n",
            "| policy_entropy     | 43.2      |\n",
            "| total_timesteps    | 15000     |\n",
            "| value_loss         | 3.93      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00955 |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 3100     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 15500    |\n",
            "| value_loss         | 56.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.79e-07 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 3200     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 16000    |\n",
            "| value_loss         | 0.652    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 3300     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 16500    |\n",
            "| value_loss         | 7.79     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.00425  |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 3400     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 17000    |\n",
            "| value_loss         | 30       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00763 |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 3500     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 17500    |\n",
            "| value_loss         | 24.2     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -0.000584 |\n",
            "| fps                | 285       |\n",
            "| nupdates           | 3600      |\n",
            "| policy_entropy     | 43.3      |\n",
            "| total_timesteps    | 18000     |\n",
            "| value_loss         | 3.07      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.00103 |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 3700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 18500    |\n",
            "| value_loss         | 2.44     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0226  |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 3800     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 19000    |\n",
            "| value_loss         | 43.3     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.314   |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 3900     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 19500    |\n",
            "| value_loss         | 1.32     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0524   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 4000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 20000    |\n",
            "| value_loss         | 59.4     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.226   |\n",
            "| fps                | 283      |\n",
            "| nupdates           | 4100     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 20500    |\n",
            "| value_loss         | 4.69     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0155   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4200     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 21000    |\n",
            "| value_loss         | 8.74     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0224   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4300     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 21500    |\n",
            "| value_loss         | 49.4     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -2.38e-07 |\n",
            "| fps                | 284       |\n",
            "| nupdates           | 4400      |\n",
            "| policy_entropy     | 43.3      |\n",
            "| total_timesteps    | 22000     |\n",
            "| value_loss         | 42.9      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0229  |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4500     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 22500    |\n",
            "| value_loss         | 15       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.252    |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4600     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 23000    |\n",
            "| value_loss         | 4.87     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.111    |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 4700     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 23500    |\n",
            "| value_loss         | 0.605    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.153   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4800     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24000    |\n",
            "| value_loss         | 3.08     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0.0376   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 4900     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 24500    |\n",
            "| value_loss         | 34.8     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.843   |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 5000     |\n",
            "| policy_entropy     | 43.2     |\n",
            "| total_timesteps    | 25000    |\n",
            "| value_loss         | 13.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0128  |\n",
            "| fps                | 284      |\n",
            "| nupdates           | 5100     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 25500    |\n",
            "| value_loss         | 2.58     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 5.96e-08 |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5200     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26000    |\n",
            "| value_loss         | 1.84     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0592  |\n",
            "| fps                | 285      |\n",
            "| nupdates           | 5300     |\n",
            "| policy_entropy     | 43.3     |\n",
            "| total_timesteps    | 26500    |\n",
            "| value_loss         | 2.93     |\n",
            "---------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 286       |\n",
            "| nupdates           | 5400      |\n",
            "| policy_entropy     | 43.4      |\n",
            "| total_timesteps    | 27000     |\n",
            "| value_loss         | 1.25      |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| explained_variance | -1.19e-07 |\n",
            "| fps                | 286       |\n",
            "| nupdates           | 5500      |\n",
            "| policy_entropy     | 43.4      |\n",
            "| total_timesteps    | 27500     |\n",
            "| value_loss         | 32.8      |\n",
            "----------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 286      |\n",
            "| nupdates           | 5600     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 28000    |\n",
            "| value_loss         | 0.222    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 5700     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 28500    |\n",
            "| value_loss         | 20.7     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | -0.0414  |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 5800     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 29000    |\n",
            "| value_loss         | 0.899    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 1.19e-07 |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 5900     |\n",
            "| policy_entropy     | 43.5     |\n",
            "| total_timesteps    | 29500    |\n",
            "| value_loss         | 1.84     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| explained_variance | 0        |\n",
            "| fps                | 287      |\n",
            "| nupdates           | 6000     |\n",
            "| policy_entropy     | 43.4     |\n",
            "| total_timesteps    | 30000    |\n",
            "| value_loss         | 10.3     |\n",
            "---------------------------------\n",
            "Training time (A2C):  1.7482445200284322  minutes\n",
            "======A2C Validation from:  20200106 to  20200406\n",
            "A2C Sharpe Ratio:  -0.43159343143025697\n",
            "======PPO Training========\n",
            "Training time (PPO):  5.9943885405858355  minutes\n",
            "======PPO Validation from:  20200106 to  20200406\n",
            "PPO Sharpe Ratio:  -0.4467472660216231\n",
            "======DDPG Training========\n",
            "Training time (DDPG):  1.0392054041226706  minutes\n",
            "======DDPG Validation from:  20200106 to  20200406\n",
            "======Trading from:  20200406 to  20200707\n",
            "previous_total_asset:1542687.4396004216\n",
            "end_total_asset:1543743.3439944005\n",
            "total_reward:1055.9043939788826\n",
            "total_cost:  520.2103515420055\n",
            "total trades:  113\n",
            "Sharpe:  0.07904974567401839\n",
            "Ensemble Strategy took:  152.15672432581584  minutes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}